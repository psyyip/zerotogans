<h1>ZerotoGANS Project - COVID-19 X-Ray Detection</h1>

<h2>Project Description</h2>
In this course project, I pick a COVID-19 Detection X-Ray Dataset and apply the concepts I learned in the course to train deep learning classification models end-to-end with PyTorch, experimenting with different hyperparameters & metrics. 

<h2>Purpose of the Project</h2>
Purpose of this project is to read the lung X-Ray images to classify the X-ray images to lungs belonging to COVID-19, bacterial, viral pneumonia patients or normal persons.

<h2>Dataset</h2>
Following the new Coronavirus(COVID-19) outbreak, data scientists are looking for patterns and methods to speed up testing and diagnostics. This dataset consists of X-ray(PA-CXR) images of COVID-19,bacterial and viral pneumonia patients and Normal people.

The dataset has three directories- TrainData,ValData and NonAugmentedTrain.<br><br>

<b>TrainData</b> - 2,083 images of shape (300,400,1) of Normal(901),COVID-19(60+60),Bacterial Pneumonia(650), Viral Pneumonia(412)<br>
<b>ValDat</b>a - 988 images of variable shapes of Normal(450),COVID-19(9),ViralPneumonia(205),Bacterial Pneumonia(324)<br>
<b>Non Augmented Data</b> - Same Data (2,002) as TrainData but not augmented<br><br>

As the Non Augmented Data is same as TrainData but not augmented, I decided not to use it for training or evaluation.

<h2>Acknowledgements</h2>
Github user ieee8023 for COVID-19 X-Rays Paul Mooney for Pneumonia Dataset

<h2>Evaluation</h2>
I use accuracy to assessment the performance of the model. At the same time, the training and validation losses (from cross entropy) can tell us if there losses are big and if there is any overfitting.

<h2>Architecture and Hyper-parameters</h2>
Initially, I used the simple CNN model we learnt in lecture 4. But I found that the results do not look good (accuracy) and unstable. Therefore, I use the ResNet9 and the new skills I learnt in lecture 5.

I tried different combination of the following so as to improve the accuracy and reduce losses:<br>
<li>more epochs
<li>higher or lower training rates
<li>Adding Relu layers
<li>Adding more Conv2d layers
<li>Adding Relu layers
<li>Increasing more channels
<li>Increasing batch size
<li>Randomized data augmentations
<li>Learning rate scheduling
<li>Weight decaly
<li>Gradient clipping
<br><br>
I did not apply Channel-wise data normalization because my images are all greyscale.<br><br>

I found that the batch size cannot be higher than 80 and the number of channels cannot be higher than 128, otherwise, my notebook will be out of memory.

No matter how much I tried, the best accuracy is still within 65% to 70%. I think it is probably due to the small training dataset I got.

<h2>Outputs</h2>
I print out the training & validation loss, accuracy and learning rates to illustration the results. <br><br>

By the end of my notebook, I also save the weights of my model such that next time, I can upload the saved model weights and re-use the model without training it again.
  
The current accuracy is not ideal. Next time, I will use bigger dataset and see if the accuracy will further improve.

This is the link to my <a href="https://jovian.ml/yip47173/ZerotoGANS-Project">notebook</a>
